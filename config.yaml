# ===============================
# Neural / Ember configuration
# ===============================

# -------------------------------
# Server (FastAPI / CORS / UI)
# -------------------------------
server:
  host: "127.0.0.1"
  port: 8000
  cors_origins:
    - "*"
  # If hosting the static UI elsewhere, set this (shown in footer)
  public_base_url: null

logging:
  level: "INFO"          # DEBUG | INFO | WARNING | ERROR
  json: false            # true = structured JSON logs
  file: null             # e.g., "logs/server.log" (creates dirs automatically)

# -------------------------------
# Model (local LLM runtime)
# -------------------------------
model:
  model_dir: "models"                           # directory for model weights
  model_path: "llama-3.1-8b-instruct.Q4_K_M.gguf"  # default model
  n_ctx: 4096                                   # context length (tokens)
  n_threads: null                               # CPU threads (null = auto)
  n_gpu_layers: null                            # GPU layers (null = auto, 0 = CPU only)
  use_mmap: true                                # memory-mapped I/O

  # Sampling parameters
  temperature: 0.7
  top_p: 0.95
  top_k: 50
  repeat_penalty: 1.1
  max_new_tokens: 512

  # Advanced
  seed: 42
  batch_size: 1
  gpu_split: null

# -------------------------------
# Memory (vector store + policies)
# -------------------------------
memory:
  data_dir: "data"                              # directory for embeddings & history
  max_context_snippets: 6                       # snippets retrieved per query
  embed_model: "sentence-transformers/all-MiniLM-L6-v2"
  embed_cache_size: 10000
  auto_trim: true
  soft_cap_entries: 50000
  trim_target_entries: 40000

# -------------------------------
# Identity (anchors + memories)
# -------------------------------
identity:
  anchors_file: "src/identity/anchors.yaml"
  anchors_per_query: 2
  anchor_injection: "every_query"

  system_prompt: |
    You are Neural (Ember), a helpful assistant with memory, identity,
    and contextual awareness. Answer clearly, truthfully, and with warmth.

  # ---- Memories integration ----
  # (A) docs/memory.md → embedded into vector DB
  memories_chunk_chars: 1200

  # (B) Anchors.yaml → policy rotation
  policy_defaults:
    mem_mode: "anchors"        # "anchors" | "off"
    mem_max_per_query: 2
    mem_chunk_chars: 1200

# -------------------------------
# Web tools (search + fetch)
# -------------------------------
web:
  allowed_domains:
    - example.com
    - arxiv.org
    - youtube.com
  allowed_file_extensions:
    - ""
    - .html
    - .txt
    - .pdf
    - .md
  max_file_size_mb: 25
  fetch_timeout: 15
  user_agent: "Ember/1.0 (+https://github.com/zackbrooks84/Neural)"
  search_provider: "duckduckgo"
  provider_opts: {}

# -------------------------------
# UI (docs/ static client hints)
# -------------------------------
ui:
  default_api_base: "http://localhost:8000"
  show_advanced: true